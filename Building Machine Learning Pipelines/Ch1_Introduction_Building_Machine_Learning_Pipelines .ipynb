{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch1_Introduction_Building_Machine_Learning_Pipelines.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1VmPZ3A2oOn",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 1. Introduction\n",
        "\n",
        "## Building Machine Learning Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8r4o85b3e4X",
        "colab_type": "text"
      },
      "source": [
        "Benefits of Machine Learning Pipelines:\n",
        "\n",
        "1) Ability to focus on new models and not maintain existing models\n",
        "\n",
        "2) Prevents bugs by keeping the same preprocessing steps through different iterations\n",
        "\n",
        "3) Creates a paper trail of hyperparameters and datasets used as well as model metrics\n",
        "\n",
        "4) Standardization across models results in efficiency across teams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_gPyMc9bbd",
        "colab_type": "text"
      },
      "source": [
        "Life Cycle of a Machine Learning Model:\n",
        "\n",
        "1) Data ingestion/versioning\n",
        "\n",
        "*   process the data in a version the following components can digest\n",
        "\n",
        "2) Data validation\n",
        "*   Check that distribution and categories are as expected\n",
        "*   Check whether classes are balanced or imbalanced\n",
        "\n",
        "3) Data preprocessing\n",
        "*   Ex: one-hot encoding, tokenizing text\n",
        "*   modifying preprocessing invalidates prior data and requires updating the entire pipeline\n",
        "\n",
        "4) Model training and tuning\n",
        "\n",
        "*   Train a model to take inputs and predict output with the lowest error possible\n",
        "\n",
        "5) Model analysis\n",
        "\n",
        "*   Go past just using accuracy or loss to determine optimal model; go on to use other metrics such as precision, recall, and AUC\n",
        "*   Evaluate model's dependency on features used in training\n",
        "\n",
        "6) Model versioning\n",
        "\n",
        "*   Document all inputs into a new model version such as hyperparameters, datasets, and architecture\n",
        "\n",
        "7) Model deployment\n",
        "\n",
        "*   Use API interfaces like representational state transfer (REST) or remote procedure call (RPC) protocols\n",
        "*   Host multiple models at the same time and perform A/B tests\n",
        "*   Model servers can allow you to update a model version without redploying your application\n",
        "\n",
        "8) Model feedback\n",
        "\n",
        "*   Captures information about the performance of the model and possibly new data\n",
        "*   This part may use a human and not be automated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhEshunNG393",
        "colab_type": "text"
      },
      "source": [
        "Tools used to orchestrate the machine learning pileline:\n",
        "\n",
        "*   Apache Beam\n",
        "*   Apache airflow\n",
        "*   Kubeflow Pipe.ines for Kubernetes infrastructure\n",
        "*   TensorFlow ML MetadataStore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_dmopniHZ9r",
        "colab_type": "text"
      },
      "source": [
        "\"Because of the two conditions (being directed and acyclic), pipeline graphs are called directed acyclic graphs (DAGs). You will discover DAGs are a central concept behind most workflow tools.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD6MvhaKH9qU",
        "colab_type": "text"
      },
      "source": [
        "Example Project:\n",
        "\"To follow along with this book, we have created an example project using open source data. The dataset is a collection of consumer complaints about financial products in the United States, and it contains a mixture of structured data (categorical/numeric data) and unstructured data (text). The data is taken from the Consumer Finance Protection Bureau.\"\n",
        "\n",
        "\"The core of our example deep learning project is the model generated by the function get_model in the components/module.py script of our example project. The model predicts whether a consumer disputed a complaint using the following features:\n",
        "\n",
        "The financial product\n",
        "\n",
        "The subproduct\n",
        "\n",
        "The company’s response to the complaint\n",
        "\n",
        "The issue that the consumer complained about\n",
        "\n",
        "The US state\n",
        "\n",
        "The zip code\n",
        "\n",
        "The text of the complaint (the narrative)\n",
        "\n",
        "For the purpose of building the machine learning pipeline, we assume that the model architecture design is done and we won’t modify the model. We discuss the model architecture in more detail in Chapter 6. But for this book, the model architecture is a very minor point. This book is all about what you can do with your model once you have it.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdbgF-KoIj4M",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNO17mVT910L",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}