{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch2_BuildingMachineLearningPipelines.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ndiJ_mmq7kB",
        "colab_type": "text"
      },
      "source": [
        "## Chapter 2. Introduction to TensorFlow Extended"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGkLt7z7rB1g",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   The TensorFlow Extended (TFX) library has all the components for a machine learning pipeline.\n",
        "*   Pipeline can be executed using an orchestrator (Airflow or Kubeflow Pipelines)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt9n94wyr04U",
        "colab_type": "text"
      },
      "source": [
        "Apache Beam is an open source tool for defining and executing data-processing jobs:\n",
        "1.   runs under the hood of many TFX components to carry out processing steps like data validation or data preprocessing\n",
        "2.   can be used as a pipeline orchestrator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXIxm64wsJMy",
        "colab_type": "text"
      },
      "source": [
        "In 2019, Google then published the open source glue code containing all the required pipeline components to tie the libraries together and to automatically create pipeline definitions for orchestration tools like Apache Airflow, Apache Beam, and Kubeflow Pipelines.\n",
        "\n",
        "*   Data ingestion with ExampleGen\n",
        "*   Data validation with StatisticsGen, SchemaGen, and the ExampleValidator\n",
        "*   Data preprocessing with Transform\n",
        "*   Model training with Trainer\n",
        "*   Checking for previously trained models with ResolverNode\n",
        "*   Model analysis and validation with Evaluator\n",
        "*   Model deployments with Pusher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrKkZlNBtx1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#install TFX\n",
        "$ pip install tfx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEGRoY54t3V3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load packages\n",
        "import tensorflow_data_validation as tfdv\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "\n",
        "\n",
        "#or import TFX component\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import Evaluator\n",
        "from tfx.components import Transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js14QVqauF68",
        "colab_type": "text"
      },
      "source": [
        "Internal Components of a Machine Learning Pipeline Component\n",
        "1.   Driver: receives input by querying the metadata store\n",
        "2.   Executor: performs the action of the component\n",
        "3.   Publisher: Saves the output or final result metadata in the MetadataStore\n",
        "\n",
        "Artifacts - inputs and outputs. Ex: raw input data, preprocessed data, and trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndO7BS67vv-b",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   The components of TFX “communicate” through metadata; instead of passing artifacts directly between the pipeline components, the components consume and publish references to pipeline artifacts.\n",
        "*   One advantage of passing the metadata between components instead of the direct artifacts is that the information can be centrally stored.\n",
        "\n",
        "\n",
        "Currently, MLMD supports three types of backends:\n",
        "1.   In-memory database (via SQLite)\n",
        "2.   SQLite\n",
        "3.   MySQL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBH5-jHXxE41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import required packages\n",
        "import tensorflow as tf\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import \\\n",
        "    InteractiveContext\n",
        "\n",
        "\n",
        "#create a context object which handles component execution and displays the component’s artifacts\n",
        "context = InteractiveContext()\n",
        "\n",
        "\n",
        "#execute each component object\n",
        "from tfx.components import StatisticsGen\n",
        "\n",
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "context.run(statistics_gen)\n",
        "\n",
        "\n",
        "#inspect features of the dataset\n",
        "context.show(statistics_gen.outputs['statistics'])\n",
        "\n",
        "\n",
        "#access artifact properties\n",
        "for artifact in statistics_gen.outputs['statistics'].get():\n",
        "    print(artifact.uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T02AfjLBx76M",
        "colab_type": "text"
      },
      "source": [
        "Alternatives to TFX\n",
        "\n",
        "*   AirBnb - Aerosolve\n",
        "*   Stripe - Railyard\n",
        "*   Spotify - Luigi\n",
        "*   Uber - Micheangelo\n",
        "*   Netflix - Metaflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TrOkbjVyOeg",
        "colab_type": "text"
      },
      "source": [
        "\"Apache Beam offers you an open source, vendor-agnostic way to describe data processing steps that then can be executed on various environments. Since it is incredibly versatile, Apache Beam can be used to describe batch processes, streaming operations, and data pipelines. In fact, TFX relies on Apache Beam and uses it under the hood for a variety of components (e.g., TensorFlow Transform or TensorFlow Data Validation).\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qpny2NPylxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#install Apache Beam\n",
        "$ pip install apache-beam\n",
        "\n",
        "#install Apache Beam for Google Cloud\n",
        "$ pip install 'apache-beam[gcp]'\n",
        "\n",
        "#Install Apache Beam for AWS\n",
        "$ pip install 'apache-beam[boto]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cikTpqHyzZJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Apache Beam Introduction\n",
        "\n",
        "import re\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "\n",
        "input_file = \"gs://dataflow-samples/shakespeare/kinglear.txt\"   #text is stored in a Google Cloud bucket\n",
        "output_file = \"/tmp/output.txt\"\n",
        "\n",
        "# Define pipeline options object.\n",
        "pipeline_options = PipelineOptions()\n",
        "\n",
        "with beam.Pipeline(options=pipeline_options) as p:  #set up the apache beam pipeline\n",
        "    # Read the text file or file pattern into a PCollection.\n",
        "    lines = p | ReadFromText(input_file)  #create a data collection by reading the textfile\n",
        "\n",
        "    # Count the occurrences of each word.\n",
        "    counts = (  #perform the transformations on the collection\n",
        "        lines\n",
        "        | 'Split' >> beam.FlatMap(lambda x: re.findall(r'[A-Za-z\\']+', x))\n",
        "        | 'PairWithOne' >> beam.Map(lambda x: (x, 1))\n",
        "        | 'GroupAndSum' >> beam.CombinePerKey(sum))\n",
        "\n",
        "    # Format the counts into a PCollection of strings.\n",
        "    def format_result(word_count):\n",
        "        (word, count) = word_count\n",
        "        return \"{}: {}\".format(word, count)\n",
        "\n",
        "    output = counts | 'Format' >> beam.Map(format_result)\n",
        "\n",
        "    # Write the output using a \"Write\" transform that has side effects.\n",
        "    output | WriteToText(output_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9Yd4Uh90Df6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#If you want to execute this pipeline on different Apache Beam runners like Apache Spark or Apache Flink, \n",
        "#you will need to set the pipeline configurations through the pipeline_options object\n",
        "\n",
        "python basic_pipeline.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snPXa4AuyjSS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}