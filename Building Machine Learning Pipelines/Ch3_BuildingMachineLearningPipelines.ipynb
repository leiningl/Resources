{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch3_BuildingMachineLearningPipelines.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF1jA8h7xs2A",
        "colab_type": "text"
      },
      "source": [
        "## Chapter 3. Data Ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pvucZekx0ij",
        "colab_type": "text"
      },
      "source": [
        "Steps:\n",
        "1.   Read data files or request data from an external service (cloud service)\n",
        "2.   Divide data (train, validation, test sets)\n",
        "3.   Convert datasets into TFRecord files containing data as tf.Example data structures - these are binary files which can be digested efficiently\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEHgcEMjzO_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ingestion of a folder containing CSV data\n",
        "\n",
        "import os\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.utils.dsl_utils import external_input\n",
        "\n",
        "base_dir = os.getcwd()\n",
        "data_dir = os.path.join(os.pardir, \"data\")\n",
        "examples = external_input(os.path.join(base_dir, data_dir))   # define data path\n",
        "example_gen = CsvExampleGen(input=examples)   # instantiate the pipeline component\n",
        "\n",
        "context.run(example_gen)  # execute the component interactively\n",
        "\n",
        "\n",
        "# when data can't be expressed efficiently as a CSV, convert the dataset to TFRecord data structures\n",
        "# and load TFRecord files with ImportExampleGen component"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I37AQLAE0J5-",
        "colab_type": "text"
      },
      "source": [
        "To load new file types into your pipeline, override the executor_class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CieO024w7407",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper functions to reduce code redundancy.\n",
        "# these convert the data records into the correct data structure used by tf.Example\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBSbJN3Y8Yjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ExampleGen component reads files from cloud (Google Cloud or AWS)\n",
        "# this will also require properly setting up cloud specific credentials\n",
        "\n",
        "examples = external_input(\"gs://example_compliance_data/\")\n",
        "example_gen = CsvExampleGen(input=examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKJM_ACb8uSm",
        "colab_type": "text"
      },
      "source": [
        "Components to ingest datasets directly from databases\n",
        "1.   BigQueryExampleGen - queries data from BigQuery tables\n",
        "2.   PrestoExampleGen - queries data from Presto databases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTeahwna9so8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splits data into training, evaluation, and test sets (6:2:2), hash_buckets sets the ratios\n",
        "\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.proto import example_gen_pb2\n",
        "from tfx.utils.dsl_utils import external_input\n",
        "\n",
        "base_dir = os.getcwd()\n",
        "data_dir = os.path.join(os.pardir, \"data\")\n",
        "output = example_gen_pb2.Output(\n",
        "    split_config=example_gen_pb2.SplitConfig(splits=[   # define preferred splits\n",
        "        example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=6),   # specify the ratio\n",
        "        example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2),\n",
        "        example_gen_pb2.SplitConfig.Split(name='test', hash_buckets=2)\n",
        "    ]))\n",
        "\n",
        "examples = external_input(os.path.join(base_dir, data_dir))\n",
        "example_gen = CsvExampleGen(input=examples, output_config=output)    #add output_config argument\n",
        "\n",
        "context.run(example_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS_6mnaJ-FCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspect artifacts by printing list of artifacts\n",
        "\n",
        "for artifact in example_gen.outputs['examples'].get():\n",
        "    print(artifact)\n",
        "\n",
        "Artifact(type_name: ExamplesPath,\n",
        "    uri: /path/to/CsvExampleGen/examples/1/train/, split: train, id: 2)\n",
        "Artifact(type_name: ExamplesPath,\n",
        "    uri: /path/to/CsvExampleGen/examples/1/eval/, split: eval, id: 3)\n",
        "Artifact(type_name: ExamplesPath,\n",
        "    uri: /path/to/CsvExampleGen/examples/1/test/, split: test, id: 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2lTYGp9-axz",
        "colab_type": "text"
      },
      "source": [
        "\"One of the significant use cases for machine learning pipelines is that we can update our machine learning models when new data becomes available. For this scenario, the ExampleGen component allows us to use spans. Think of a span as a snapshot of data. Every hour, day, or week, a batch extract, transform, load (ETL) process could make such a data snapshot and create a new span.\n",
        "\n",
        "A span can replicate the existing data records. As shown in the following, export-1 contains the data from the previous export-0 as well as newly created records that were added since the export-0 export:\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrFsr71t-umo",
        "colab_type": "text"
      },
      "source": [
        "\"In machine learning pipelines, we want to track the produced models together with the used datasets, which were used to train the machine learning model. To do this, it is useful to version our datasets.\n",
        "\n",
        "Data versioning allows us to track the ingested data in more detail. This means that we not only store the file name and path of the ingested data in the ML MetadataStore (because itâ€™s currently supported by the TFX components) but also that we track more metainformation about the raw dataset, such as a hash of the ingested data. Such version tracking would allow us to verify that the dataset used during the training is still the dataset at a later point in time. Such a feature is critical for end-to-end ML reproducibility.\n",
        "\n",
        "However, such a feature is currently not supported by the TFX ExampleGen component. If you would like to version your datasets, you can use third-party data versioning tools and version the data before the datasets are ingested into the pipeline. Unfortunately, none of the available tools will write the metadata information to the TFX ML MetadataStore directly.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHuIdskj-yeU",
        "colab_type": "text"
      },
      "source": [
        "Third party data versioning\n",
        "*   Data Version Control (DVC)\n",
        "*   Pachyderm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_8KLg0w_vxL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}